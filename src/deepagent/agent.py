from typing import AsyncGenerator

from langchain.chat_models.base import BaseChatModel
from langchain.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks.manager import dispatch_custom_event
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode

from deepagent.state import AgentState
from deepagent.chunk import Chunk, ChunkType
from deepagent.prompts import PromptFactory
from deepagent.plan import Plan
from deepagent.research import ResearchResult
from deepagent.tools import generate_tools_description


class DeepAgent:
    """
    Agente profundo que razona y act√∫a seg√∫n un plan definido para llevar a cabo la petici√≥n del usuario.

    Construye un grafo de estados con los siguientes nodos:
    - "init": inicializa el estado del agente con el mensaje de sistema.
    - "planner": invoca el modelo para generar un plan de acci√≥n.
    - "researcher": realiza investigaciones basadas en el plan.
    - "summarizer": resume los resultados de la investigaci√≥n y genera una respuesta final.
    - "executor": ejecuta herramientas solicitadas por el modelo y retorna sus resultados.

    Args:
        model: instancia del modelo conversacional (por ejemplo, ``ChatOpenAI``).
        domain: descripci√≥n del dominio de conocimiento del agente.
        tone: tono de comunicaci√≥n del agente.
        tools: lista de herramientas compatibles con LangChain a exponer al modelo.
        verbose: si es True, imprime mensajes de depuraci√≥n y resultados de herramientas.
    """


    def __init__(
        self,
        model: BaseChatModel,
        domain: str,
        tone: str,
        tools: list[tool],
        verbose: bool = False,
    ):
        """
        Inicializa el agente con un modelo, herramientas y un prompt de sistema.
        Args:
            model: instancia del modelo conversacional (por ejemplo, ``ChatOpenAI``).
            domain: descripci√≥n del dominio de conocimiento del agente.
            tone: tono de comunicaci√≥n del agente.
            tools: lista de herramientas compatibles con LangChain a exponer al modelo.
            verbose: si es True, imprime mensajes de depuraci√≥n y resultados de herramientas.
        """

        self.tools = tools
        self.domain = domain
        self.tone = tone

        # Descripci√≥n de las herramientas disponibles
        self.tools_description = generate_tools_description(self.tools)

        # Indica si el agente debe operar en modo verbose
        self.verbose = verbose

        # Configuraci√≥n del grafo
        self.config : RunnableConfig = {
            "recursionLimit": 50,      # L√≠mite de recursi√≥n para evitar bucles infinitos en el grafo (para que no est√© infinitamente dando vueltas)
            "configurable": {
                "thread_id": "1"        # Identificador del hilo de conversaci√≥n (en este caso, 1 agente s√≥lo puede mantener una conversaci√≥n a la vez)
            },
        }

        # Construcci√≥n del grafo de estados del agente
        self.graph = self.__build_graph()

        # Asociar las herramientas al modelo
        self.model = model

        # Construir el prompt de sistema
        self.system_prompt = SystemMessage(content=PromptFactory.render(
            "system",
            {
                "domain": self.domain,
                "tools": self.tools_description,
                "tone": self.tone,
            },
        ))


    def __build_graph(self) -> CompiledStateGraph:
        """
        Construye y compila el grafo de estados del agente.
        Returns:
            El grafo de estados compilado.
        """
        # Construcci√≥n del grafo de estados del agente
        graph = StateGraph(AgentState)

        # Definici√≥n de nodos del grafo
        graph.add_node("planner", self.__plan)          # Nodo de planificaci√≥n
        graph.add_node("researcher", self.__research)   # Nodo de investigaci√≥n
        graph.add_node("tools", ToolNode(self.tools))   # Nodo de ejecuci√≥n de herramientas

        # graph.add_node("action", self.__take_action)              # Nodo de ejecuci√≥n de herramientas (acciones)

        # Definici√≥n de aristas del grafo
        graph.add_edge("planner", "researcher")  # Desde el modelo, ir a investigar
        graph.add_edge("researcher", END)  # Si el modelo no pide herramientas, terminar
        ##graph.add_conditional_edges(
        ##    "llm",                                              # La arista condicional sale del nodo "llm"
        ##    self.__exists_action,                                 # Funci√≥n que decide si se debe ir al nodo de acci√≥n o terminar
        ##    {True: "action", False: END},
        ##)                                                       # Si el modelo decide llamar a una herramienta, ir al nodo de acci√≥n; si no, terminar
        ##graph.add_edge("action", "llm")                         # Despu√©s de ejecutar una acci√≥n, volver al modelo

        # Definici√≥n del punto de entrada del grafo
        graph.set_entry_point("planner")

        # Compilaci√≥n del grafo para su ejecuci√≥n con checkpointer en memoria (guarda el estado en memoria)
        return graph.compile(checkpointer=InMemorySaver())


    def __plan(self, state: AgentState, config: RunnableConfig) -> AgentState:
        """
        Genera un plan de acci√≥n basado en la consulta del usuario.
        Args:
            state: estado actual con la consulta del usuario.
            config: configuraci√≥n del runnable con callbacks.
        Returns:
            Un estado actualizado con el plan de acci√≥n.
        """

        # Emitir evento de inicio de planificaci√≥n
        dispatch_custom_event("planning_started", {"status": "planning_started"}, config=config)

        # Construir el mensaje de sistema para la planificaci√≥n
        plan_prompt = SystemMessage(content=PromptFactory.render("plan", {"domain": self.domain}))

        # Invocar el modelo para obtener el plan
        structured_model : BaseChatModel[Plan] = self.model.with_structured_output(Plan)
        plan : Plan = structured_model.invoke([
            self.system_prompt,
            plan_prompt, 
            self.human_query
        ], config=config)

        # Emitir evento de planificaci√≥n completada
        dispatch_custom_event("planning_completed", {
            "status": "planning_completed",
            "steps": plan.steps,
            "steps_count": len(plan.steps)
        }, config=config)

        return {
            **state, 
            "plan": plan,
            "messages": state.get("messages", []) + [AIMessage(content=f"He generado un plan de acci√≥n con {len(plan.steps)} pasos: {plan}")]  
        }

    def __research(self, state: AgentState, config: RunnableConfig) -> AgentState:
        """
        Realiza una investigaci√≥n basada en el plan de acci√≥n.
        Args:
            state: estado actual con el plan de acci√≥n.
        Returns:
            Un estado actualizado con los resultados de la investigaci√≥n.
        """
        plan: Plan = state["plan"]
        step : str = plan.next_step()
        if step:

            # Emitir evento de planificaci√≥n completada
            dispatch_custom_event("research_started", {
                "status": "research_started",
                "step": step,
            }, config=config)

            reasearch_prompt = SystemMessage(content=PromptFactory.render(
                "research", 
                {
                    "domain": self.domain
                }
            ))

            messages = [
                self.system_prompt,
                reasearch_prompt,
                HumanMessage(content=f"Paso del plan a analizar: {step}."),
            ]

            structured_model : BaseChatModel[ResearchResult] = self.model.with_structured_output(ResearchResult)
            intent = structured_model.invoke(messages, config=config)

            # Emitir evento de planificaci√≥n completada
            dispatch_custom_event("research_completed", {
                "status": "research_completed",
                "step": step,
                "intent": intent.dict(),
            }, config=config)

        return {"plan": plan}


    def __summarize(self, state: AgentState) -> AgentState:
        """
        Resume los resultados de la investigaci√≥n y genera una respuesta final.
        Args:
            state: estado actual con todos los resultados de la investigaci√≥n.
        Returns:
            Un estado actualizado con la respuesta final.
        """
        if self.verbose:
            print("üß© Resumiendo resultados finales...")

        # TODO implementar el resumen final

        sys = SystemMessage(
            content=self.system_prompt
            + "\nResume en 5-8 l√≠neas lo importante para seguir ejecutando. Devuelve texto."
        )
        out: AIMessage = self.model.invoke([sys] + state.get("messages", []))

        # estrategia simple: guardamos el resumen como un SystemMessage y recortamos
        summary = out.content
        new_messages = [SystemMessage(content=f"RESUMEN CONTEXTO:\n{summary}")]
        return {**state, "messages": new_messages, "scratch": state.get("scratch", {})}


        if self.verbose:
            print("‚úÖ Resumen completado")

        return state

    def __route(self, state: AgentState) -> AgentState:
        """
        Decide la siguiente acci√≥n del agente: continuar investigando o sintetizar resultados.
        Args:
            state: estado actual del agente.
        Returns:
            Un estado actualizado despu√©s de decidir la siguiente acci√≥n.
        """
        plan: Plan = state.get("plan")
        if not plan:
            return state

        if plan.current_step < len(plan.steps):
            if self.verbose:
                print(
                    f"üîÑ Continuando investigaci√≥n - Paso {plan.current_step + 1}/{len(plan.steps)}"
                )
            return self.__research(state)
        else:
            if self.verbose:
                print("üèÅ Investigaci√≥n completada, procediendo a s√≠ntesis")
            return self.__finalize(state)
        

    def __finalize(self, state: AgentState) -> AgentState:
        """
        Sintetiza los resultados de la investigaci√≥n en una respuesta final.
        Args:
            state: estado actual con todos los resultados de la investigaci√≥n.
        Returns:
            Un estado actualizado con la respuesta final.
        """
        if self.verbose:
            print("üß© Sintetizando resultados finales...")

        # TODO implementar la s√≠ntesis final

        if self.verbose:
            print("‚úÖ S√≠ntesis completada")

        return state


    # ------------------------------------------
    # Nodos condicionales
    # ------------------------------------------

    def __need_summarize(self, state: AgentState) -> bool:
        """
        Determina si el agente debe proceder a la s√≠ntesis de resultados.
        Args:
            state: estado actual del agente.
        Returns:
            True si no hay m√°s pasos por ejecutar y se debe sintetizar, False en caso contrario.
        """
        return len(state.get("messages", [])) > 30

    def __need_tools(self, state: AgentState) -> bool:
        """
        Comprueba si el modelo ha solicitado la ejecuci√≥n de herramientas.
        Args:
            state: estado actual con el historial de mensajes.
        Returns:
            ``True`` si el √∫ltimo mensaje del modelo incluye ``tool_calls``; en caso contrario ``False``.
        """
        # √öltimo mensaje generado por el modelo
        last = state.get("messages", [])[-1]
        # Devuelve True si hay llamadas a herramientas en el √∫ltimo mensaje
        return isinstance(last, AIMessage) and getattr(last, "tool_calls", None)
    

    def __need_more_steps(self, state: AgentState) -> bool:
        """
        Determina si debe continuar con la investigaci√≥n o proceder a la s√≠ntesis.
        Args:
            state: estado actual del agente.
        Returns:
            True si hay m√°s pasos por ejecutar, False en caso contrario.
        """
        plan: Plan = state.get("plan")
        if not plan:
            return False

        # Continuar si hay m√°s pasos por ejecutar
        should_continue = plan.current_step < len(plan.steps)

        if self.verbose:
            if should_continue:
                print(
                    f"üîÑ Continuando investigaci√≥n - Paso {plan.current_step + 1}/{len(plan.steps)}"
                )
            else:
                print("üèÅ Investigaci√≥n completada, procediendo a s√≠ntesis")

        return should_continue

    # ------------------------------------------
    # Ejecuci√≥n del agente
    # ------------------------------------------

    async def invoke(self, query: str) -> AsyncGenerator[Chunk, None]:
        """
        Formula una pregunta al agente y devuelve la √∫ltima respuesta.
        Args:
            query: texto de la consulta del usuario.
        Returns:
            Generador as√≠ncrono que produce fragmentos de la respuesta final del agente.
        """
        self.human_query = HumanMessage(content=query)

        initial_state = {
            "messages": [ 
                self.system_prompt,
                self.human_query
            ]
        }

        # Ejecuta el grafo de modo as√≠ncrono y obtiene los eventos
        events = self.graph.astream_events(
            input=initial_state,
            config=self.config,
        )

        async for event in events:
            # Obtiene el tipo de evento
            kind = event["event"]
            chunk : Chunk = None    # Fragmento de respuesta que emite el agente
            match kind:

                # Solo mostrar eventos del modelo del nodo researcher
                case "on_chat_model_stream":
                    # Filtrar por el nodo que queremos mostrar
                    lg_node = event['metadata'].get('langgraph_node', "")
                    content = event["data"]["chunk"].content
                    match lg_node:
                        case "researcher" | "planner":
                            chunk = Chunk(type=ChunkType.THINKING, content=content)
                        case "finalizer":
                            chunk = Chunk(type=ChunkType.TEXT, content=content)

                # Manejar eventos personalizados de planificaci√≥n
                case "on_custom_event":
                    event_name = event.get("name", "")
                    data = event.get("data", {})
                    
                    match event_name:
                        case "planning_started":
                            chunk = Chunk(type=ChunkType.THINKING, content="üß† Generando plan de acci√≥n...\n")
                        case "planning_completed":
                            steps_count = data.get("steps_count", 0)
                            chunk = Chunk(type=ChunkType.THINKING, content=f"\n‚úÖ Plan generado con {steps_count} pasos\n")
                        case "research_started":
                            step = data.get("step", "")
                            chunk = Chunk(type=ChunkType.THINKING, content=f"\nüîé Investigando paso: {step}\n")
                        case "research_completed":
                            intent = data.get("intent", {})
                            chunk = Chunk(type=ChunkType.THINKING, content=f"\n‚úÖ Investigaci√≥n completada. Intent: {intent}\n")
                        case _:
                            # Otros eventos personalizados
                            if self.verbose:
                                chunk = Chunk(type=ChunkType.THINKING, content=f"[{event_name}]: {data}\n")

                case _:                    
                    #print(f"üîî Evento no manejado: {kind} en nodo {node_name}")
                    pass

            if chunk:
                yield chunk

    # ------------------------------------------
    # Utilidades de depuraci√≥n
    # ------------------------------------------

    def print_graph(self) -> None:
        """Imprime una representaci√≥n del grafo del agente."""
        print(self.graph.get_graph().draw_mermaid())
